








from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
import pandas as pd
import numpy as np

df = pd.read_csv('../Dataset/archive/WA_Fn-UseC_-HR-Employee-Attrition.csv')

X = df.drop(['Attrition', 'EmployeeNumber', 'Over18', 'EmployeeCount', 'StandardHours'], axis=1)
y = df['Attrition']

col_int32 = ['DailyRate', 'MonthlyIncome', 'MonthlyRate']
col_int16 = ['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 
             'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'NumCompaniesWorked', 
             'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 
             'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 
             'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 
             'YearsSinceLastPromotion', 'YearsWithCurrManager']

# ép kiểu
X[col_int32] = X[col_int32].astype('int32')
X[col_int16] = X[col_int16].astype('int16')

X_origin = X.copy()
y_origin = y.copy()

# One-hot cho các cột nominal
one_hot_encoder = OneHotEncoder(
    sparse_output=False,
    handle_unknown='ignore',
    drop='first'
)

encode_cols = [
    'BusinessTravel', 'Department', 'EducationField',
    'Gender', 'JobRole', 'MaritalStatus', 'OverTime'
]

X_encoded_array = one_hot_encoder.fit_transform(X_origin[encode_cols])
encoded_columns = one_hot_encoder.get_feature_names_out(encode_cols)

X_encoded_df = pd.DataFrame(
    X_encoded_array, 
    columns=encoded_columns,
    index=X_origin.index
)

# Gộp data sau one-hot
X_encode = pd.concat([X_origin.drop(columns=encode_cols), X_encoded_df], axis=1)

# Encode y
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_origin)

# Continuous columns cần scale
continuous_data = [
    'Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome',
    'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike',
    'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany',
    'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'
]

# Standard Scaler
scaler = StandardScaler()
X_scaled_df = pd.DataFrame(
    scaler.fit_transform(X_encode[continuous_data]),
    columns=continuous_data,
    index=X_encode.index
).astype('float32')

# Gộp cùng dữ liệu còn lại
X_final = pd.concat(
    [
        X_encode.drop(columns=continuous_data), 
        X_scaled_df                       
    ],
    axis=1
)

# One-hot columns cũng ép về float32
one_hot_cols = X_encoded_df.columns
X_final[one_hot_cols] = X_final[one_hot_cols].astype('float32')

# Có 2 loại dữ liệu là X_origin là X ban đầu đã được One Hot và X đã được làm sạch, chuẩn hóa.
X = X_final
X_origin = X_encode


X_origin


X_origin.info()





float_cols = X_origin.select_dtypes(include='float64').columns
X_origin[float_cols] = X_origin[float_cols].astype('int8')



X_origin.info()





import matplotlib.pyplot as plt
import numpy as np
import warnings
warnings.filterwarnings("ignore")








LAYERS = [
    (25,),   
    (50,),
    (50, 25), 
    (75, 50),
    (100, 75),
    (100, 75, 50)
]

TEST_SIZES = [0.2, 0.3, 0.4]

LEARNING_RATES = [0.0005, 0.001]

ALPHAS = [0.01]

OTPS = ['adam']






def plot_true_vs_pred_line(
    y_true,
    y_pred,
    bins=50,
    title="So sánh phân bố y thực và y dự đoán"
):
    import numpy as np
    import matplotlib.pyplot as plt
    
    counts_true, bin_edges = np.histogram(y_true, bins=bins)
    counts_pred, _ = np.histogram(y_pred, bins=bin_edges)
    centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    plt.plot(centers, counts_true, label="Giá trị thực", linewidth=2)
    plt.plot(centers, counts_pred, label="Giá trị dự đoán", linestyle="--", linewidth=2)

    plt.xlabel("Giá trị")
    plt.ylabel("Tần suất")
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()






def validate(X=None, y=None, command='No comment', std=True):
    global LAYERS, TEST_SIZES, LEARNING_RATES, ALPHAS

    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    from sklearn.neural_network import MLPRegressor
    from sklearn.preprocessing import StandardScaler

    continuous_data = [
        'DailyRate', 'MonthlyRate',
        'Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 
        'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'NumCompaniesWorked', 
        'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 
        'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 
        'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 
        'YearsSinceLastPromotion', 'YearsWithCurrManager'
    ]
    results = []

    for otp in OTPS:
        for layer in LAYERS:
            for test_size in TEST_SIZES:
                for lr in LEARNING_RATES:
                    for alpha in ALPHAS:
    
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, y, test_size=test_size, random_state=42
                        )
    
                        if std:
                            scaler = StandardScaler()
                            X_train = scaler.fit_transform(X_train[continuous_data])
                            X_test = scaler.transform(X_test[continuous_data])
    
                        model = MLPRegressor(
                            hidden_layer_sizes=layer,
                            learning_rate_init=lr,
                            learning_rate='adaptive',
                            alpha=alpha,
                            max_iter=1000,
                            early_stopping=True,
                            random_state=42,
                            solver=otp
                        )
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
    
                        mae = mean_absolute_error(y_test, y_pred)
                        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                        r2 = r2_score(y_test, y_pred)
    
                        results.append({
                            "layers": layer,
                            "test_size": test_size,
                            "learning_rate": lr,
                            "alpha": alpha,
                            "MAE": mae,
                            "RMSE": rmse,
                            "R2": r2,
                            "Solver": otp
                        })
    
                        for i in range(3):
                            print()
                        print(
                            f"Layers={layer}, solver = {otp} test_size={test_size}, "
                            f"lr={lr}, alpha={alpha} | "
                            f"RMSE={rmse:.2f}, R2={r2:.3f}"
                        )
    
                        plot_true_vs_pred_line(y_test, y_pred)

    return results



def validate_redution_data(X=None, y=None, command='No comment', std=True):
    global LAYERS, TEST_SIZES, LEARNING_RATES, ALPHAS

    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    from sklearn.neural_network import MLPRegressor
    from sklearn.preprocessing import StandardScaler
    results = []

    for otp in OTPS:
        for layer in LAYERS:
            for test_size in TEST_SIZES:
                for lr in LEARNING_RATES:
                    for alpha in ALPHAS:
    
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, y, test_size=test_size, random_state=42
                        )
    
                        if std:
                            scaler = StandardScaler()
                            X_train = scaler.fit_transform(X_train)
                            X_test = scaler.transform(X_test)
    
                        model = MLPRegressor(
                            hidden_layer_sizes=layer,
                            learning_rate_init=lr,
                            learning_rate='adaptive',
                            alpha=alpha,
                            max_iter=1000,
                            early_stopping=True,
                            random_state=42,
                            solver=otp
                        )
                        model.fit(X_train, y_train)
                        y_pred = model.predict(X_test)
    
                        mae = mean_absolute_error(y_test, y_pred)
                        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                        r2 = r2_score(y_test, y_pred)
    
                        results.append({
                            "layers": layer,
                            "test_size": test_size,
                            "learning_rate": lr,
                            "alpha": alpha,
                            "MAE": mae,
                            "RMSE": rmse,
                            "R2": r2,
                            "Solver": otp
                        })
    
                        for i in range(3):
                            print()
                        print(
                            f"Layers={layer}, solver = {otp} test_size={test_size}, "
                            f"lr={lr}, alpha={alpha} | "
                            f"RMSE={rmse:.2f}, R2={r2:.3f}"
                        )
    
                        plot_true_vs_pred_line(y_test, y_pred)

    return results



def validate_sgd(X=None, y=None, command='No comment', std=True):
    LAYERS = [
        (25,),   
        (50,),
        (50, 25), 
        (75, 50),
        (100, 75),
        (100, 75, 50)
    ]
    
    TEST_SIZES = [0.2, 0.3]
    
    LEARNING_RATES = [0.001]
    
    ALPHAS = [0.1]

    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    from sklearn.neural_network import MLPRegressor
    from sklearn.preprocessing import StandardScaler

    continuous_data = [
        'DailyRate', 'MonthlyRate',
        'Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 
        'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'NumCompaniesWorked', 
        'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 
        'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 
        'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 
        'YearsSinceLastPromotion', 'YearsWithCurrManager'
    ]
    results = []

    for layer in LAYERS:
        for test_size in TEST_SIZES:
            for lr in LEARNING_RATES:
                for alpha in ALPHAS:

                    X_train, X_test, y_train, y_test = train_test_split(
                        X, y, test_size=test_size, random_state=42
                    )

                    if std:
                        scaler = StandardScaler()
                        X_train = scaler.fit_transform(X_train[continuous_data])
                        X_test = scaler.transform(X_test[continuous_data])

                    model = MLPRegressor(
                        hidden_layer_sizes=layer,
                        solver='sgd',
                        learning_rate_init=lr,
                        learning_rate='adaptive',
                        momentum=0.9,
                        nesterovs_momentum=True,
                        alpha=alpha,
                        max_iter=3000,
                        early_stopping=True
                    )

                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)

                    mae = mean_absolute_error(y_test, y_pred)
                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                    r2 = r2_score(y_test, y_pred)

                    results.append({
                        "layers": layer,
                        "test_size": test_size,
                        "learning_rate": lr,
                        "alpha": alpha,
                        "MAE": mae,
                        "RMSE": rmse,
                        "R2": r2,
                        "Solver": 'sgd'
                    })

                    for i in range(3):
                        print()
                    print(
                        f"Layers={layer}, solver = sgd test_size={test_size}, "
                        f"lr={lr}, alpha={alpha} | "
                        f"RMSE={rmse:.2f}, R2={r2:.3f}"
                    )

                    plot_true_vs_pred_line(y_test, y_pred)

    return results



def plot_two_series(
    series1,
    series2,
    label1="Series 1",
    label2="Series 2",
    title="Comparison of Two Series",
    xlabel="Index",
    ylabel="Value"
):
    plt.figure(figsize=(10, 5))

    plt.plot(
        series1,
        marker='o',
        linewidth=2,
        label=label1
    )

    plt.plot(
        series2,
        marker='s',
        linewidth=2,
        label=label2
    )

    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)

    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.6)

    plt.tight_layout()
    plt.show()









X_reg = X_origin[[col for col in X_origin.columns if col != 'MonthlyIncome']]
y_reg = X_origin['MonthlyIncome']


X_reg.head()


y_reg.head()





plt.hist(y_reg, bins=50, color="mediumseagreen", edgecolor="black", alpha=0.6)
plt.xlabel("Đô la")
plt.ylabel("Tần suất")
plt.title("Biểu đồ thu nhập hàng tháng của nhân viên (Monthly Income)")
plt.show()


import numpy as np
import matplotlib.pyplot as plt

counts, bin_edges = np.histogram(y_reg, bins=50)

bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

plt.plot(bin_centers, counts, marker='o')
plt.xlabel("Thu nhập hàng tháng (USD)")
plt.ylabel("Tần suất")
plt.title("Phân bố thu nhập hàng tháng (dạng đường)")
plt.grid(True, alpha=0.3)
plt.show()















result_sgd = validate_sgd(X = X_reg, y = y_reg, command = 'Chạy với dữ liệu được chuẩn hóa sử dụng Gradient Descent!')


df_sgd = pd.DataFrame(result_sgd)
df_sgd











result_no_std = validate(X = X_reg, y = y_reg, command = 'Chạy với dữ liệu không được chuẩn hóa!', std = False)


df_no_std = pd.DataFrame(result_no_std)


df_no_std











result_std = validate(X = X_reg, y = y_reg, command = 'Chạy với dữ liệu được chuẩn hóa!')


df_std = pd.DataFrame(result_std) 


df_std














X_reg.head()


y_reg.head()








n_components = int(len(X_reg.columns) / 3)
n_components


from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_reg)

pca = PCA(n_components=14)
X_reg_pca = pca.fit_transform(X_scaled)


# Phương sai riêng lẻ
explained_var = pca.explained_variance_ratio_

# Phương sai tích lũy
cumulative_var = np.cumsum(explained_var)

print(cumulative_var)
print(explained_var)


X_reg_pca


result_pca_1 = validate_redution_data(X = X_reg_pca, y = y_reg, std = False)


df_pca_1 = pd.DataFrame(result_pca_1)
df_pca_1





plt.figure(figsize=(7,5))
plt.plot(
    range(1, len(cumulative_var) + 1),
    cumulative_var,
    marker='o'
)
plt.axhline(y=0.95, linestyle='--', label='95% variance')
plt.xlabel('Số thành phần chính (Principal Components)')
plt.ylabel('Phương sai tích lũy')
plt.title('Biểu đồ phương sai tích lũy của PCA')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


















df_std.sort_values(
    by=["R2", "MAE", "RMSE"],
    ascending=[False, True, True]
).head(5)


df_sgd.sort_values(
    by=["R2", "MAE", "RMSE"],
    ascending=[False, True, True]
).head(5)











plot_two_series(df_std['R2'], df_no_std['R2'], label1= 'df_std', label2='df_no_std',title = 'So sánh chỉ số R2 của mô hình với dữ liệu chưa giảm chiều, đã giảm chiều')


plot_two_series(df_std['RMSE'], df_no_std['RMSE'], label1= 'df_std', label2='df_no_std',title = 'So sánh chỉ số RMSE của mô hình với dữ liệu chưa giảm chiều, đã giảm chiều')








df_std.head()


df_pca_1.head()


plot_two_series(df_std['R2'], df_pca_1['R2'], label1= 'df_std', label2='df_pca',title = 'So sánh chỉ số R2 của mô hình với dữ liệu chưa giảm chiều, đã giảm chiều')


plot_two_series(df_std['RMSE'], df_pca_1['RMSE'], label1= 'RMSE', label2='RMSE',title = 'So sánh chỉ số RMSE của mô hình với dữ liệu chưa giảm chiều, đã giảm chiều')
