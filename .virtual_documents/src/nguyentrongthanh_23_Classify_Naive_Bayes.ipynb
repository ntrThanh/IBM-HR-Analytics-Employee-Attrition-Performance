





from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
import pandas as pd
import numpy as np

df = pd.read_csv('../Dataset/archive/WA_Fn-UseC_-HR-Employee-Attrition.csv')

X = df.drop(['Attrition', 'EmployeeNumber', 'Over18', 'EmployeeCount', 'StandardHours'], axis=1)
y = df['Attrition']

col_int32 = ['DailyRate', 'MonthlyIncome', 'MonthlyRate']
col_int16 = ['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 
             'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'NumCompaniesWorked', 
             'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 
             'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 
             'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 
             'YearsSinceLastPromotion', 'YearsWithCurrManager']

# ép kiểu
X[col_int32] = X[col_int32].astype('int32')
X[col_int16] = X[col_int16].astype('int16')

X_origin = X.copy()
y_origin = y.copy()

# One-hot cho các cột nominal
one_hot_encoder = OneHotEncoder(
    sparse_output=False,
    handle_unknown='ignore',
    drop='first'
)

encode_cols = [
    'BusinessTravel', 'Department', 'EducationField',
    'Gender', 'JobRole', 'MaritalStatus', 'OverTime'
]

X_encoded_array = one_hot_encoder.fit_transform(X_origin[encode_cols])
encoded_columns = one_hot_encoder.get_feature_names_out(encode_cols)

X_encoded_df = pd.DataFrame(
    X_encoded_array, 
    columns=encoded_columns,
    index=X_origin.index
)

# Gộp data sau one-hot
X_encode = pd.concat([X_origin.drop(columns=encode_cols), X_encoded_df], axis=1)

# Encode y
label_encoder = LabelEncoder()
y_encode = label_encoder.fit_transform(y_origin)

# One-hot columns cũng ép về float32
one_hot_cols = X_encoded_df.columns
X_encode[one_hot_cols] = X_encode[one_hot_cols].astype('float32')



print(X_encode.shape)


X_encode.info()


X_encode


y_encode





import numpy as np
import pandas as pd

class My_GaussianNB:
    def __init__(self):
        self.classes = None
        self.mean = {}
        self.var = {}
        self.priors = {}

    def fit(self, X, y):
        self.classes = np.unique(y)
        n_samples = X.shape[0]

        for c in self.classes:
            X_c = X[y == c]
            self.priors[c] = len(X_c) / n_samples
            self.mean[c] = np.mean(X_c, axis=0)
            self.var[c] = np.var(X_c, axis=0) + 1e-9

    def _gaussian_pdf(self, x, mean, var):
        return np.exp(-((x - mean) ** 2) / (2 * var)) / np.sqrt(2 * np.pi * var)

    def predict_log_proba(self, X):
        n_samples = X.shape[0]
        n_classes = len(self.classes)
        log_probs = np.zeros((n_samples, n_classes))

        for idx, c in enumerate(self.classes):
            log_prior = np.log(self.priors[c])
            pdf = self._gaussian_pdf(X, self.mean[c], self.var[c])
            log_likelihood = np.sum(np.log(pdf + 1e-9), axis=1)
            log_probs[:, idx] = log_prior + log_likelihood

        return log_probs

    def predict(self, X):
        log_probs = self.predict_log_proba(X)
        return self.classes[np.argmax(log_probs, axis=1)]

class My_BernoulliNB:
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        self.classes = None
        self.feature_prob = {}  # P(feature=1|class)
        self.priors = {}
    
    def fit(self, X, y):
        self.classes = np.unique(y)
        n_samples = X.shape[0]
        n_features = X.shape[1]
        
        for c in self.classes:
            X_c = X[y == c]
            n_c = len(X_c)
            
            # Prior probability P(class)
            self.priors[c] = n_c / n_samples
            
            # Feature probability P(feature=1|class) với Laplace smoothing
            # Smoothing: (count + alpha) / (total + 2*alpha)
            # 2*alpha vì Bernoulli có 2 giá trị (0 và 1)
            feature_count = np.sum(X_c, axis=0)
            self.feature_prob[c] = (feature_count + self.alpha) / (n_c + 2 * self.alpha)
        
        return self
    
    def predict_log_proba(self, X):
        n_samples = X.shape[0]
        n_classes = len(self.classes)
        log_probs = np.zeros((n_samples, n_classes))
        
        for idx, c in enumerate(self.classes):
            log_prior = np.log(self.priors[c])
            
            # Log likelihood cho Bernoulli:
            # log P(X|c) = Σ[x_i * log(p_i) + (1-x_i) * log(1-p_i)]
            p = self.feature_prob[c]
            log_p = np.log(p)
            log_1_p = np.log(1 - p)
            
            # Vectorized calculation
            log_likelihood = X @ log_p + (1 - X) @ log_1_p
            log_probs[:, idx] = log_prior + log_likelihood
        
        return log_probs
    
    # không cộng thêm xác suất tiên nghiệm (log_prior)
    def predict_log_proba_2(self, X):
        n_samples = X.shape[0]
        n_classes = len(self.classes)
        log_probs = np.zeros((n_samples, n_classes))
        
        for idx, c in enumerate(self.classes):
            log_prior = np.log(self.priors[c])
            
            # Log likelihood cho Bernoulli:
            # log P(X|c) = Σ[x_i * log(p_i) + (1-x_i) * log(1-p_i)]
            p = self.feature_prob[c]
            log_p = np.log(p)
            log_1_p = np.log(1 - p)
            
            # Vectorized calculation
            log_likelihood = X @ log_p + (1 - X) @ log_1_p
            log_probs[:, idx] = log_likelihood
        
        return log_probs
    
    def predict(self, X):
        log_probs = self.predict_log_proba(X)
        return self.classes[np.argmax(log_probs, axis=1)]



# Function dùng để train mô hình kết hợp: Gaussian và Bernoulli

from imblearn.over_sampling import SMOTE, SMOTENC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

def fit_and_evaluate_hybrid(gnb, bnb, 
                           X_train, X_test, 
                           y_train, y_test, 
                           sca = True,
                           use_smote=False,
                           description="Hybrid Gaussian–Bernoulli"):
    print("="*60)
    print(f"ĐANG HUẤN LUYỆN: {description}")


    try:
        X_train = X_train.copy()
        X_test  = X_test.copy()

        continuous_data = [
        'Age', 'DailyRate', 'DistanceFromHome', 
        'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',
        'PercentSalaryHike', 'TotalWorkingYears',
        'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole',
        'YearsSinceLastPromotion', 'YearsWithCurrManager',

        # Đây là các dữ liệu phân loại có thứ tự ta sẽ chuẩn hóa cùng dữ liệu liên tục và đưa vô gnb
        'Education',
        'EnvironmentSatisfaction',
        'JobInvolvement',
        'JobLevel',
        'JobSatisfaction',
        'PerformanceRating',
        'RelationshipSatisfaction',
        'StockOptionLevel',
        'WorkLifeBalance'
        ]
        
        X_train_num = X_train[continuous_data].to_numpy()
        X_train_cat = X_train.drop(continuous_data, axis=1).to_numpy()

        X_test_num = X_test[continuous_data].to_numpy()
        X_test_cat = X_test.drop(continuous_data, axis=1).to_numpy()

        if sca:
            scaler = StandardScaler()
            X_train_num = scaler.fit_transform(X_train_num)
            X_test_num = scaler.transform(X_test_num)  

        # Apply SMOTE nếu được bật
        if use_smote:
            cat_indices = [i for i in range(X_train_num.shape[1], X_train_num.shape[1] + X_train_cat.shape[1])]
            smote_nc = SMOTENC(categorical_features=cat_indices, random_state=42)
            
            X_combined = np.hstack([X_train_num, X_train_cat])
            X_res, y_res = smote_nc.fit_resample(X_combined, y_train)
            
            # Tách lại thành num + cat
            X_train_num = X_res[:, :X_train_num.shape[1]]
            X_train_cat = X_res[:, X_train_num.shape[1]:]
            y_train = y_res    

        # Fit mô hình
        gnb.fit(X_train_num, y_train)
        bnb.fit(X_train_cat, y_train)

        # Dự đoán cho tập train
        # Predict log xác suất của từng model
        # Lưu ý: Dùng predict_log_proba cho BNB mặc dù bị tính 2 lần xác suất tiên nghiệm nhưng lại cho kết quả tốt hơn 
        log_proba_gnb_train = gnb.predict_log_proba(X_train_num)
        log_proba_bnb_train = bnb.predict_log_proba(X_train_cat)

        # Cộng log xác suất (hybrid)
        combined_log_train = log_proba_gnb_train + log_proba_bnb_train

        # Lấy class dự đoán
        y_pred_train = gnb.classes[np.argmax(combined_log_train, axis=1)]

        # Dự đoán cho tập test
        # Predict log xác suất của từng model
        log_proba_gnb_test = gnb.predict_log_proba(X_test_num)
        log_proba_bnb_test = bnb.predict_log_proba(X_test_cat)

        # Cộng log xác suất (hybrid)
        combined_log_test = log_proba_gnb_test + log_proba_bnb_test

        # Lấy class dự đoán
        y_pred_test = gnb.classes[np.argmax(combined_log_test, axis=1)]

        # Đánh giá
        print(f"\n Train Accuracy: {accuracy_score(y_train, y_pred_train):.4f}")
        print(f"Test Accuracy: {accuracy_score(y_test, y_pred_test):.4f}")
        print("\n Classification Report:\n", classification_report(y_test, y_pred_test, zero_division=0))
        print("\n Confusion Matrix:\n", confusion_matrix(y_test, y_pred_test))

    except Exception as e:
        print(f"\n LỖI: {e}")

    print("="*60 + "\n")



# Function dùng để train mô hình chỉ dùng Gausssian

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
def fit_and_evaluate_gaussian_only(gnb, X_train, X_test, y_train, y_test, sca=True, reduce_name=None, description="Only Gaussian"):
    print("="*60)
    print(f"ĐANG HUẤN LUYỆN: {description}")

    try:

        if sca:
            continuous_data = [
            'Age', 'DailyRate', 'DistanceFromHome', 'EmployeeCount',
            'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',
            'PercentSalaryHike', 'StandardHours', 'TotalWorkingYears',
            'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole',
            'YearsSinceLastPromotion', 'YearsWithCurrManager'
            ]
        cont_cols = [col for col in continuous_data if col in X_train.columns]

        scaler = StandardScaler()
        X_train[cont_cols] = scaler.fit_transform(X_train[cont_cols])
        X_test[cont_cols] = scaler.transform(X_test[cont_cols])

        if reduce_name == 'PCA':
            model_rd = PCA(n_components=6)
            X_train = model_rd.fit_transform(X_train)
            X_test = model_rd.transform(X_test)
        elif reduce_name == 'LDA':
            model_rd = LDA(n_components=1)
            X_train = model_rd.fit_transform(X_train, y_train)
            X_test = model_rd.transform(X_test)    

        gnb.fit(X_train, y_train)

        # Dự đoán
        y_pred_train = gnb.predict(X_train)
        y_pred_test = gnb.predict(X_test)

        # Đánh giá kết quả
        print(f"\n Train Accuracy: {accuracy_score(y_train, y_pred_train):.4f}")
        print(f" Test Accuracy: {accuracy_score(y_test, y_pred_test):.4f}")
        print("\n Classification Report (Test):\n", classification_report(y_test, y_pred_test, zero_division=0))
        print("\n Confusion Matrix (Test):\n", confusion_matrix(y_test, y_pred_test))

    except Exception as e:
        print(f"\n LỖI: {e}")
        import traceback
        traceback.print_exc()

    print("="*60 + "\n")



import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_recall_curve, auc
from sklearn.preprocessing import StandardScaler

def visualize_hybrid_results(gnb_model, bnb_model, 
                             X_train, X_test, y_train, y_test, 
                             title="Hybrid Model"):
    
    X_train = X_train.copy()
    X_test  = X_test.copy()

    continuous_data = [
        'Age', 'DailyRate', 'DistanceFromHome', 
        'HourlyRate', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked',
        'PercentSalaryHike', 'TotalWorkingYears',
        'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole',
        'YearsSinceLastPromotion', 'YearsWithCurrManager',

        # Đây là các dữ liệu phân loại có thứ tự ta sẽ chuẩn hóa cùng dữ liệu liên tục và đưa vô gnb
        'Education',
        'EnvironmentSatisfaction',
        'JobInvolvement',
        'JobLevel',
        'JobSatisfaction',
        'PerformanceRating',
        'RelationshipSatisfaction',
        'StockOptionLevel',
        'WorkLifeBalance'
        
    ]
        
    X_train_num = X_train[continuous_data].to_numpy()
    X_train_cat = X_train.drop(continuous_data, axis=1).to_numpy()

    X_test_num = X_test[continuous_data].to_numpy()
    X_test_cat = X_test.drop(continuous_data, axis=1).to_numpy()
    
    scaler = StandardScaler()
    X_train_num = scaler.fit_transform(X_train_num)
    X_test_num = scaler.transform(X_test_num)  

    gnb_model.fit(X_train_num, y_train)
    bnb_model.fit(X_train_cat, y_train)

    # Tính Log Proba
    # Lưu ý: Dùng predict_log_proba cho BNB mặc dù bị tính 2 lần xác suất tiên nghiệm nhưng lại cho kết quả tốt hơn 
    # Dự đoán Class
    log_proba_gnb_test = gnb_model.predict_log_proba(X_test_num)
    log_proba_bnb_test = bnb_model.predict_log_proba(X_test_cat)
    combined_log_test = log_proba_gnb_test + log_proba_bnb_test
    y_pred_test = gnb_model.classes[np.argmax(combined_log_test, axis=1)]
    
    probs = np.exp(combined_log_test)
    probs_sum = np.sum(probs, axis=1, keepdims=True)
    probs = probs / probs_sum 
    y_scores = probs[:, 1] 

    # --- TRỰC QUAN HÓA ---
    plt.figure(figsize=(20, 6))

    # Biểu đồ 1: Heatmap Confusion Matrix
    
    plt.subplot(1, 3, 1)
    cm = confusion_matrix(y_test, y_pred_test)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=gnb_model.classes, yticklabels=gnb_model.classes)
    plt.title(f'1. Heatmap Confusion Matrix\n({title})')
    plt.xlabel('Dự đoán')
    plt.ylabel('Thực tế')

    # Biểu đồ 2: So sánh phân phối thực tế vs dự đoán
    
    plt.subplot(1, 3, 2)
    comparison_df = pd.DataFrame({
        'Loại': np.concatenate([['Thực tế'] * len(y_test), ['Dự đoán'] * len(y_pred_test)]),
        'Giá trị': np.concatenate([y_test, y_pred_test])
    })
    sns.countplot(x='Giá trị', hue='Loại', data=comparison_df, palette='muted')
    plt.title('2. Phân phối Thực tế vs Dự đoán')
    plt.xlabel('Lớp (0: Ở lại, 1: Nghỉ việc)')
    plt.ylabel('Số lượng mẫu')

    # Biểu đồ 3: Đường cong Precision-Recall
    
    plt.subplot(1, 3, 3)
    precision, recall, _ = precision_recall_curve(y_test, y_scores)
    pr_auc = auc(recall, precision)
    plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR AUC = {pr_auc:.2f}')
    plt.fill_between(recall, precision, alpha=0.2, color='darkorange')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('3. Precision-Recall Curve')
    plt.legend(loc="lower left")

    plt.tight_layout()
    plt.show()





from sklearn.model_selection import train_test_split
X_copy_encode, y_copy_encode = X_encode.copy(), y_encode.copy()








my_gnb = My_GaussianNB()
my_bnb = My_BernoulliNB()


fit_and_evaluate_hybrid(my_gnb, my_bnb, 
                        *train_test_split(X_copy_encode, y_copy_encode,test_size=0.2,random_state=42),
                        sca=True,
                        description="Naive gnb+bnb (train/test: 4/1)")


fit_and_evaluate_hybrid(my_gnb, my_bnb, 
                        *train_test_split(X_copy_encode, y_copy_encode,test_size=0.3,random_state=42),
                        sca=True,
                        description="Naive gnb+bnb (train/test: 7/3)")


fit_and_evaluate_hybrid(my_gnb, my_bnb, 
                        *train_test_split(X_copy_encode, y_copy_encode,test_size=0.4,random_state=42),
                        sca=True,
                        description="Naive gnb+bnb (train/test: 6/4)")





my_gnb_2 = My_GaussianNB()


fit_and_evaluate_gaussian_only(my_gnb_2,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.2,random_state=42),
                               description="Only Gaussian (train/test: 4/1)")


fit_and_evaluate_gaussian_only(my_gnb_2,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.3,random_state=42),
                               description="Only Gaussian (train/test: 7/3)")


fit_and_evaluate_gaussian_only(my_gnb_2,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.4,random_state=42),
                               description="Only Gaussian (train/test: 6/4)")








my_gnb_sm = My_GaussianNB()
my_bnb_sm = My_BernoulliNB()


fit_and_evaluate_hybrid(my_gnb, my_bnb, 
                        *train_test_split(X_copy_encode, y_copy_encode,test_size=0.2,random_state=42),
                        sca=True,
                        use_smote=True,
                        description="Naive gnb+bnb (train/test: 4/1)")


fit_and_evaluate_hybrid(my_gnb, my_bnb, 
                        *train_test_split(X_copy_encode, y_copy_encode,test_size=0.3,random_state=42),
                        sca=True,
                        use_smote=True,
                        description="Naive gnb+bnb (train/test: 7/3)")


fit_and_evaluate_hybrid(my_gnb, my_bnb, 
                        *train_test_split(X_copy_encode, y_copy_encode,test_size=0.4,random_state=42),
                        sca=True,
                        use_smote=True,
                        description="Naive gnb+bnb (train/test: 6/4)")











my_gnb_reduce = My_GaussianNB()





fit_and_evaluate_gaussian_only(my_gnb_reduce,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.2,random_state=42),
                               reduce_name='PCA',
                               description="Only Gaussian PCA 6 (train/test: 4/1)")


fit_and_evaluate_gaussian_only(my_gnb_reduce,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.3,random_state=42),
                               reduce_name='PCA',
                               description="Only Gaussian PCA 6 (train/test: 7/3)")


fit_and_evaluate_gaussian_only(my_gnb_reduce,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.4,random_state=42),
                               reduce_name='PCA',
                               description="Only Gaussian PCA 6 (train/test: 6/4)")





fit_and_evaluate_gaussian_only(my_gnb_reduce,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.2,random_state=42),
                               reduce_name='LDA',
                               description="Only Gaussian LDA 1 (train/test: 4/1)")


fit_and_evaluate_gaussian_only(my_gnb_reduce,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.3,random_state=42),
                               reduce_name='LDA',
                               description="Only Gaussian LDA 1 (train/test: 7/3)")


fit_and_evaluate_gaussian_only(my_gnb_reduce,
                               *train_test_split(X_copy_encode, y_copy_encode,test_size=0.4,random_state=42),
                               reduce_name='LDA',
                               description="Only Gaussian LDA 1 (train/test: 6/4)")




















X_copy_encode_2, y_copy_encode_2 = X_encode.copy(), y_encode.copy()


my_gnb_end = My_GaussianNB()
my_bnb_end = My_BernoulliNB()


visualize_hybrid_results(my_gnb_end, my_bnb_end,
                        *train_test_split(X_copy_encode_2, y_copy_encode_2,test_size=0.2,random_state=42))



