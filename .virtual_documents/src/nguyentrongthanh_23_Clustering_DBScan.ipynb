





from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
import pandas as pd
import numpy as np

df = pd.read_csv('../Dataset/archive/WA_Fn-UseC_-HR-Employee-Attrition.csv')

X = df.drop(['Attrition', 'EmployeeNumber', 'Over18', 'EmployeeCount', 'StandardHours'], axis=1)
y = df['Attrition']

col_int32 = ['DailyRate', 'MonthlyIncome', 'MonthlyRate']
col_int16 = ['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 
             'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'NumCompaniesWorked', 
             'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 
             'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 
             'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 
             'YearsSinceLastPromotion', 'YearsWithCurrManager']

# ép kiểu
X[col_int32] = X[col_int32].astype('int32')
X[col_int16] = X[col_int16].astype('int16')

X_origin = X.copy()
y_origin = y.copy()

# One-hot cho các cột nominal
one_hot_encoder = OneHotEncoder(
    sparse_output=False,
    handle_unknown='ignore',
    drop='first'
)

encode_cols = [
    'BusinessTravel', 'Department', 'EducationField',
    'Gender', 'JobRole', 'MaritalStatus', 'OverTime'
]

X_encoded_array = one_hot_encoder.fit_transform(X_origin[encode_cols])
encoded_columns = one_hot_encoder.get_feature_names_out(encode_cols)

X_encoded_df = pd.DataFrame(
    X_encoded_array, 
    columns=encoded_columns,
    index=X_origin.index
)

# Gộp data sau one-hot
X_encode = pd.concat([X_origin.drop(columns=encode_cols), X_encoded_df], axis=1)

# Encode y
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_origin)

# Continuous columns cần scale
continuous_data = [
    'Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome',
    'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike',
    'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany',
    'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'
]

# Standard Scaler
scaler = StandardScaler()
X_scaled_df = pd.DataFrame(
    scaler.fit_transform(X_encode[continuous_data]),
    columns=continuous_data,
    index=X_encode.index
).astype('float32')

# Gộp cùng dữ liệu còn lại
X_final = pd.concat(
    [
        X_encode.drop(columns=continuous_data), 
        X_scaled_df                       
    ],
    axis=1
)

# One-hot columns cũng ép về float32
one_hot_cols = X_encoded_df.columns
X_final[one_hot_cols] = X_final[one_hot_cols].astype('float32')

# Có 2 loại dữ liệu là X_origin là X ban đầu đã được One Hot và X đã được làm sạch, chuẩn hóa.
X = X_final
X_origin = X_encode





X_origin





X





import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def visualize_X(X, labels):
    X = np.array(X)
    labels = np.array(labels)
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    
    n_features = X.shape[1]

    if n_features == 1:
        plt.figure(figsize=(7,4))
        plt.scatter(X[:, 0], [0]*len(X), c=labels, s=50)
        plt.title("1D Data Visualization")
        plt.yticks([])  # ẩn trục Y
        plt.xlabel("Feature 1")
        plt.show()

    elif n_features == 2:
        plt.figure(figsize=(7,5))
        plt.scatter(X[:, 0], X[:, 1], c=labels, s=50)
        plt.title("2D Data Visualization")
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")
        plt.show()

    elif n_features == 3:
        fig = plt.figure(figsize=(7,5))
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels, s=50)
        ax.set_title("3D Data Visualization")
        ax.set_xlabel("Feature 1")
        ax.set_ylabel("Feature 2")
        ax.set_zlabel("Feature 3")
        plt.show()

    else:
        raise ValueError("Dữ liệu phải có 1, 2 hoặc 3 chiều để trực quan.")












from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []
for i in range(1, 16):   
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)


plt.plot(range(1, 16), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters (k) with standard data')
plt.ylabel('WCSS (Inertia)')
plt.show()








from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []
for i in range(1, 16):   
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_origin)
    wcss.append(kmeans.inertia_)


plt.plot(range(1, 16), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters (k) with origin data')
plt.ylabel('WCSS (Inertia)')
plt.show()











from sklearn.decomposition import PCA

pca2d = PCA(n_components = 2)
pca3d = PCA(n_components = 3)

pca5d = PCA(n_components = 5)
pca6d = PCA(n_components = 6)

X_2d = pca2d.fit_transform(X)
X_3d = pca3d.fit_transform(X)
X_5d = pca5d.fit_transform(X)
X_6d = pca6d.fit_transform(X)

visualize_X(X_2d, y)
visualize_X(X_3d, y)





import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.metrics import adjusted_rand_score



y





eps_list = [0.03, 0.05, 0.07, 0.9, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 3.0, 5.0]
min_samples_list = [3, 5, 10, 20, 30, 44, 60, 100]
metrics = ["cosine", "euclidean"]





results = []

for eps in eps_list:
    for m in min_samples_list:
        for metric in metrics:
            db = DBSCAN(eps=eps, min_samples=m, metric=metric)
            labels = db.fit_predict(X) 
            core_samples_mask = (labels != -1)
            
            y_pred_filtered = labels[core_samples_mask].astype(int) 
            y_true_filtered = y[core_samples_mask].astype(int)

            if len(np.unique(y_pred_filtered)) < 2:
                score = -1e9
            else:
                score = adjusted_rand_score(y_true_filtered, y_pred_filtered)

            results.append({
                "eps": eps,
                "min_samples": m,
                "metric": metric,
                "ARI": score
            })

df = pd.DataFrame(results)

print(df.sort_values("ARI", ascending=False))





results = []

for eps in eps_list:
    for m in min_samples_list:
        for metric in metrics:
            db = DBSCAN(eps=eps, min_samples=m, metric=metric)
            labels = db.fit_predict(X_2d) 
            core_samples_mask = (labels != -1)
            
            y_pred_filtered = labels[core_samples_mask].astype(int) 
            y_true_filtered = y[core_samples_mask].astype(int)

            if len(np.unique(y_pred_filtered)) < 2:
                score = -1e9
            else:
                score = adjusted_rand_score(y_true_filtered, y_pred_filtered)

            results.append({
                "eps": eps,
                "min_samples": m,
                "metric": metric,
                "ARI": score
            })

df = pd.DataFrame(results)

print(df.sort_values("ARI", ascending=False))





results = []

for eps in eps_list:
    for m in min_samples_list:
        for metric in metrics:
            db = DBSCAN(eps=eps, min_samples=m, metric=metric)
            labels = db.fit_predict(X_3d) 
            core_samples_mask = (labels != -1)
            
            y_pred_filtered = labels[core_samples_mask].astype(int) 
            y_true_filtered = y[core_samples_mask].astype(int)

            if len(np.unique(y_pred_filtered)) < 2:
                score = -1e9
            else:
                score = adjusted_rand_score(y_true_filtered, y_pred_filtered)

            results.append({
                "eps": eps,
                "min_samples": m,
                "metric": metric,
                "ARI": score
            })

df = pd.DataFrame(results)

print(df.sort_values("ARI", ascending=False))





results = []

for eps in eps_list:
    for m in min_samples_list:
        for metric in metrics:
            db = DBSCAN(eps=eps, min_samples=m, metric=metric)
            labels = db.fit_predict(X_5d) 
            core_samples_mask = (labels != -1)
            
            y_pred_filtered = labels[core_samples_mask].astype(int) 
            y_true_filtered = y[core_samples_mask].astype(int)

            if len(np.unique(y_pred_filtered)) < 2:
                score = -1e9
            else:
                score = adjusted_rand_score(y_true_filtered, y_pred_filtered)

            results.append({
                "eps": eps,
                "min_samples": m,
                "metric": metric,
                "ARI": score
            })

df = pd.DataFrame(results)

print(df.sort_values("ARI", ascending=False))





from sklearn.decomposition import PCA

pca90 = PCA(n_components = 0.9)

X_90_percent = pca90.fit_transform(X)


results = []

for eps in eps_list:
    for m in min_samples_list:
        for metric in metrics:
            db = DBSCAN(eps=eps, min_samples=m, metric=metric)
            labels = db.fit_predict(X_90_percent) 
            core_samples_mask = (labels != -1)
            
            y_pred_filtered = labels[core_samples_mask].astype(int) 
            y_true_filtered = y[core_samples_mask].astype(int)

            if len(np.unique(y_pred_filtered)) < 2:
                score = -1e9
            else:
                score = adjusted_rand_score(y_true_filtered, y_pred_filtered)

            results.append({
                "eps": eps,
                "min_samples": m,
                "metric": metric,
                "ARI": score
            })

df = pd.DataFrame(results)

print(df.sort_values("ARI", ascending=False))





from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda = LDA(n_components=1)
X_lda = lda.fit_transform(X, y)


results = []

for eps in eps_list:
    for m in min_samples_list:
        for metric in metrics:
            db = DBSCAN(eps=eps, min_samples=m, metric=metric)
            labels = db.fit_predict(X_lda) 
            core_samples_mask = (labels != -1)
            
            y_pred_filtered = labels[core_samples_mask].astype(int) 
            y_true_filtered = y[core_samples_mask].astype(int)

            if len(np.unique(y_pred_filtered)) < 2:
                score = -1e9
            else:
                score = adjusted_rand_score(y_true_filtered, y_pred_filtered)

            results.append({
                "eps": eps,
                "min_samples": m,
                "metric": metric,
                "ARI": score
            })

df = pd.DataFrame(results)

print(df.sort_values("ARI", ascending=False))

















import numpy as np
import matplotlib.pyplot as plt

db = DBSCAN(eps=0.05, min_samples=5, metric='euclidean')
labels = db.fit_predict(X_lda)

x = X_lda.reshape(-1)


import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm

x = X_lda.reshape(-1)
unique_labels = sorted(set(labels))  
cmap = cm.get_cmap('tab10', len(unique_labels))  # tạo cmap đủ số cluster
label2color = {lb: cmap(i) for i, lb in enumerate(unique_labels)}



plt.figure(figsize=(8, 4))

for lb in unique_labels:
    plt.hist(
        x[labels == lb],
        bins=15,
        alpha=0.6,
        color=label2color[lb],
        edgecolor="black",
        label="Noise (-1)" if lb == -1 else f"Cluster {lb}"
    )

plt.xlabel("X_lda (1D)")
plt.ylabel("Số lượng")
plt.title("Histogram DBSCAN – dữ liệu 1D")
plt.legend()
plt.tight_layout()
plt.show()






plt.figure(figsize=(10, 2))

for lb in unique_labels:
    plt.scatter(
        x[labels == lb],
        np.zeros_like(x[labels == lb]),
        color=label2color[lb],
        s=60,
        label="Noise (-1)" if lb == -1 else f"Cluster {lb}"
    )

plt.yticks([])
plt.xlabel("X_lda (1D)")
plt.title("DBSCAN clustering – 1D")
plt.legend()
plt.show()



y = np.random.uniform(-0.02, 0.02, size=len(x))

plt.figure(figsize=(10, 2))

for lb in unique_labels:
    plt.scatter(
        x[labels == lb],
        y[labels == lb],
        color=label2color[lb],
        s=60,
        label="Noise (-1)" if lb == -1 else f"Cluster {lb}"
    )

plt.yticks([])
plt.xlabel("X_lda (1D)")
plt.title("DBSCAN 1D (with jitter)")
plt.legend()
plt.show()






import numpy as np
import matplotlib.pyplot as plt

db = DBSCAN(eps=0.3, min_samples=30, metric='euclidean')
labels = db.fit_predict(X_2d)


labels



plt.figure(figsize=(6, 6))

for lb in sorted(set(labels)):
    plt.scatter(
        X_2d[labels == lb, 0],
        X_2d[labels == lb, 1],
        s=40,
        color=label2color.get(lb, 'gray'),
        label='Noise (-1)' if lb == -1 else f'Cluster {lb}'
    )

plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("DBSCAN clustering – 2D")
plt.legend()
plt.axis('equal')
plt.tight_layout()
plt.show()



core_mask = np.zeros_like(labels, dtype=bool)
core_mask[db.core_sample_indices_] = True

plt.figure(figsize=(6, 6))

for lb in sorted(set(labels)):
    color = label2color.get(lb, 'gray')

    plt.scatter(
        X_2d[(labels == lb) & core_mask, 0],
        X_2d[(labels == lb) & core_mask, 1],
        s=80,
        color=color,
        marker='o',
        label=f'Cluster {lb} (core)' if lb != -1 else 'Noise'
    )

    plt.scatter(
        X_2d[(labels == lb) & ~core_mask, 0],
        X_2d[(labels == lb) & ~core_mask, 1],
        s=30,
        color=color,
        marker='.'
    )

plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("DBSCAN 2D – core / border / noise")
plt.legend()
plt.axis('equal')
plt.tight_layout()
plt.show()









import numpy as np
import matplotlib.pyplot as plt

db = DBSCAN(eps=0.5, min_samples=20, metric='euclidean')
labels = db.fit_predict(X_3d)


import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401

X = np.asarray(X_3d)
labels = np.asarray(labels)

unique_labels = sorted(set(labels))

cmap = cm.get_cmap("tab10", len(unique_labels))
label2color = {lb: cmap(i) for i, lb in enumerate(unique_labels)}

views = [
    (25, 45), (30, 135), (30, 225), (30, 315),
    (90, 0), (90, 90), (90, 180), (90, 270),
    (0, 0), (0, 90), (0, 180), (0, 270),
    (60, 30), (60, 120), (60, 210), (60, 300),
    (-30, 45), (-30, 135), (-30, 225), (-30, 315),
]


for elev, azim in views:
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection="3d")

    for lb in unique_labels:
        mask = labels == lb

        if lb == -1:
            ax.scatter(
                X[mask, 0],
                X[mask, 1],
                X[mask, 2],
                c="lightskyblue",
                s=30,
                alpha=0.4,
                label="Noise (-1)"
            )
        else:
            ax.scatter(
                X[mask, 0],
                X[mask, 1],
                X[mask, 2],
                c=[label2color[lb]],
                s=50,
                alpha=0.8,
                label=f"Cluster {lb}"
            )

    ax.set_xlabel("Component 1")
    ax.set_ylabel("Component 2")
    ax.set_zlabel("Component 3")
    ax.set_title(f"DBSCAN clustering – 3D (elev={elev}, azim={azim})")

    ax.view_init(elev=elev, azim=azim)
    ax.legend()
    plt.tight_layout()
    plt.show()






pip install plotly


import numpy as np
import plotly.express as px
import pandas as pd
import os

labels_str = labels.astype(str)
labels_str[labels == '-1'] = "Noise (-1)" 

color_map = {
    'Noise (-1)': 'lightskyblue',
    '0': 'gold',
    '1': 'green',
    '2': 'red' 
}

df = pd.DataFrame({
    "x": X[:, 0],
    "y": X[:, 1],
    "z": X[:, 2],
    "cluster": labels_str
})

fig = px.scatter_3d(
    df,
    x="x",
    y="y",
    z="z",
    color="cluster",
    color_discrete_map=color_map, 
    opacity=0.8,
    title="DBSCAN clustering (3D interactive)",
)

fig.update_traces(marker=dict(size=4))
fig.update_layout(
    legend_title_text="Cluster",
    scene=dict(
        xaxis_title="Component 1",
        yaxis_title="Component 2",
        zaxis_title="Component 3",
    ),
)

output_file = "Clustering_DBScan_3d_Interactive.html"

if os.path.exists(output_file):
    print(f"File '{output_file}' đã tồn tại ghi đè")

fig.write_html(output_file)
print(f"Đã lưu biểu đồ 3D vào '{output_file}'")




