





from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
import pandas as pd

df = pd.read_csv('../Dataset/archive/WA_Fn-UseC_-HR-Employee-Attrition.csv')

X = df.drop(['Attrition', 'EmployeeNumber', 'Over18', 'EmployeeCount', 'StandardHours'], axis=1)
y = df['Attrition']

col_int32 = ['DailyRate', 'MonthlyIncome', 'MonthlyRate']
col_int16 = ['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate',
             'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'NumCompaniesWorked',
             'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',
             'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',
             'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',
             'YearsSinceLastPromotion', 'YearsWithCurrManager']

# ép kiểu
X[col_int32] = X[col_int32].astype('int32')
X[col_int16] = X[col_int16].astype('int16')

X_origin = X.copy()
y_origin = y.copy()

# One-hot cho các cột nominal
one_hot_encoder = OneHotEncoder(
    sparse_output=False,
    handle_unknown='ignore',
    drop='first'
)

encode_cols = [
    'BusinessTravel', 'Department', 'EducationField',
    'Gender', 'JobRole', 'MaritalStatus', 'OverTime'
]

X_encoded_array = one_hot_encoder.fit_transform(X_origin[encode_cols])
encoded_columns = one_hot_encoder.get_feature_names_out(encode_cols)

X_encoded_df = pd.DataFrame(
    X_encoded_array,
    columns=encoded_columns,
    index=X_origin.index
)

# Gộp data sau one-hot
X_encode = pd.concat([X_origin.drop(columns=encode_cols), X_encoded_df], axis=1)

# Encode y
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_origin)

# Continuous columns cần scale
continuous_data = [
    'Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome',
    'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike',
    'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany',
    'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'
]

# Standard Scaler
scaler = StandardScaler()
X_scaled_df = pd.DataFrame(
    scaler.fit_transform(X_encode[continuous_data]),
    columns=continuous_data,
    index=X_encode.index
).astype('float32')

# Gộp cùng dữ liệu còn lại
X_final = pd.concat(
    [
        X_encode.drop(columns=continuous_data),
        X_scaled_df
    ],
    axis=1
)

# One-hot columns cũng ép về float32
one_hot_cols = X_encoded_df.columns
X_final[one_hot_cols] = X_final[one_hot_cols].astype('float32')

# Có 2 loại dữ liệu là X_origin là X ban đầu đã được One Hot và X đã được làm sạch, chuẩn hóa.
X = X_final
X_origin = X_encode





X_origin





X





import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def visualize_X(X, labels):
    X = np.array(X)
    labels = np.array(labels)
    if X.ndim == 1:
        X = X.reshape(-1, 1)

    n_features = X.shape[1]

    if n_features == 1:
        plt.figure(figsize=(7,4))
        plt.scatter(X[:, 0], [0]*len(X), c=labels, s=50)
        plt.title("1D Data Visualization")
        plt.yticks([])  # ẩn trục Y
        plt.xlabel("Feature 1")
        plt.show()

    elif n_features == 2:
        plt.figure(figsize=(7,5))
        plt.scatter(X[:, 0], X[:, 1], c=labels, s=50)
        plt.title("2D Data Visualization")
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")
        plt.show()

    elif n_features == 3:
        fig = plt.figure(figsize=(7,5))
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels, s=50)
        ax.set_title("3D Data Visualization")
        ax.set_xlabel("Feature 1")
        ax.set_ylabel("Feature 2")
        ax.set_zlabel("Feature 3")
        plt.show()

    else:
        raise ValueError("Dữ liệu phải có 1, 2 hoặc 3 chiều để trực quan.")












from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []
for i in range(1, 16):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)


plt.plot(range(1, 16), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters (k) with standard data')
plt.ylabel('WCSS (Inertia)')
plt.show()








from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []
for i in range(1, 16):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_origin)
    wcss.append(kmeans.inertia_)


plt.plot(range(1, 16), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters (k) with origin data')
plt.ylabel('WCSS (Inertia)')
plt.show()











from sklearn.decomposition import PCA

pca2d = PCA(n_components = 2)
pca3d = PCA(n_components = 3)

pca5d = PCA(n_components = 5)
pca6d = PCA(n_components = 6)

X_2d = pca2d.fit_transform(X)
X_3d = pca3d.fit_transform(X)
X_5d = pca5d.fit_transform(X)
X_6d = pca6d.fit_transform(X)

visualize_X(X_2d, y)
visualize_X(X_3d, y)





import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.metrics import adjusted_rand_score



y


from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
import pandas as pd
import numpy as np

def kmeans_experiment(X, y, k_list, random_state=42):
    results = []

    for k in k_list:
        kmeans = KMeans(
            n_clusters=k,
            init="k-means++",
            n_init=10,
            random_state=random_state
        )
        y_pred = kmeans.fit_predict(X)

        ari = adjusted_rand_score(y, y_pred)

        results.append({
            "k": k,
            "ARI": ari
        })

    return pd.DataFrame(results).sort_values("ARI", ascending=False)


k_list = [2, 3, 4, 5, 6, 7, 8]





df_origin = kmeans_experiment(X, y, k_list)
print(df_origin)








df_pca2 = kmeans_experiment(X_2d, y, k_list)
print(df_pca2)








df_pca3 = kmeans_experiment(X_3d, y, k_list)
print(df_pca3)








df_pca5 = kmeans_experiment(X_5d, y, k_list)
print(df_pca5)








from sklearn.decomposition import PCA

pca90 = PCA(n_components = 0.9)

X_90_percent = pca90.fit_transform(X)
df_pca90 = kmeans_experiment(X_90_percent, y, k_list)
print(df_pca90)








from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

lda = LDA(n_components=1)
X_lda = lda.fit_transform(X, y)
df_lda = kmeans_experiment(X_lda, y, k_list)
print(df_lda)























import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from matplotlib import cm

kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_lda)

x = X_lda.reshape(-1)

unique_labels = sorted(set(labels))
cmap = cm.get_cmap("tab10", len(unique_labels))
label2color = {lb: cmap(i) for i, lb in enumerate(unique_labels)}

plt.figure(figsize=(8, 4))

for lb in unique_labels:
    plt.hist(
        x[labels == lb],
        bins=15,
        alpha=0.6,
        color=label2color[lb],
        edgecolor="black",
        label=f"Cluster {lb}"
    )

plt.xlabel("X_lda (1D)")
plt.ylabel("Số lượng")
plt.title("Histogram K-means – dữ liệu 1D (LDA)")
plt.legend()
plt.tight_layout()
plt.show()



y = np.random.uniform(-0.02, 0.02, size=len(x))

plt.figure(figsize=(10, 2))
for lb in unique_labels:
    plt.scatter(
        x[labels == lb],
        y[labels == lb],
        color=label2color[lb],
        s=60,
        label=f"Cluster {lb}"
    )

plt.yticks([])
plt.xlabel("X_lda (1D)")
plt.title("K-means clustering – 1D (LDA, jitter)")
plt.legend()
plt.show()









from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np

kmeans = KMeans(n_clusters=7, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_2d)

unique_labels = sorted(set(labels))
cmap = cm.get_cmap("tab10", len(unique_labels))
label2color = {lb: cmap(i) for i, lb in enumerate(unique_labels)}

plt.figure(figsize=(6, 6))

for lb in unique_labels:
    plt.scatter(
        X_2d[labels == lb, 0],
        X_2d[labels == lb, 1],
        s=40,
        color=label2color[lb],
        label=f"Cluster {lb}"
    )

plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.title("K-means clustering – PCA 2D")
plt.legend()
plt.axis("equal")
plt.tight_layout()
plt.show()









from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from matplotlib import cm
import numpy as np

kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_3d)

X = np.asarray(X_3d)
unique_labels = sorted(set(labels))

cmap = cm.get_cmap("tab10", len(unique_labels))
label2color = {lb: cmap(i) for i, lb in enumerate(unique_labels)}

fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection="3d")

for lb in unique_labels:
    ax.scatter(
        X[labels == lb, 0],
        X[labels == lb, 1],
        X[labels == lb, 2],
        c=[label2color[lb]],
        s=50,
        alpha=0.8,
        label=f"Cluster {lb}"
    )

ax.set_xlabel("Component 1")
ax.set_ylabel("Component 2")
ax.set_zlabel("Component 3")
ax.set_title("K-means clustering – PCA 3D")
ax.legend()
plt.tight_layout()
plt.show()




