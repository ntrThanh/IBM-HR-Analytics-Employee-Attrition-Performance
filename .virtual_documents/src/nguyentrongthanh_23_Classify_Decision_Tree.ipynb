





from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
import pandas as pd

df = pd.read_csv('../Dataset/archive/WA_Fn-UseC_-HR-Employee-Attrition.csv')

X = df.drop(['Attrition', 'EmployeeNumber', 'Over18', 'EmployeeCount', 'StandardHours'], axis=1)
y = df['Attrition']

col_int32 = ['DailyRate', 'MonthlyIncome', 'MonthlyRate']
col_int16 = ['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate',
             'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'NumCompaniesWorked',
             'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',
             'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',
             'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',
             'YearsSinceLastPromotion', 'YearsWithCurrManager']

# ép kiểu
X[col_int32] = X[col_int32].astype('int32')
X[col_int16] = X[col_int16].astype('int16')

X_origin = X.copy()
y_origin = y.copy()

# One-hot cho các cột nominal
one_hot_encoder = OneHotEncoder(
    sparse_output=False,
    handle_unknown='ignore',
    drop='first'
)

encode_cols = [
    'BusinessTravel', 'Department', 'EducationField',
    'Gender', 'JobRole', 'MaritalStatus', 'OverTime'
]

X_encoded_array = one_hot_encoder.fit_transform(X_origin[encode_cols])
encoded_columns = one_hot_encoder.get_feature_names_out(encode_cols)

X_encoded_df = pd.DataFrame(
    X_encoded_array,
    columns=encoded_columns,
    index=X_origin.index
)

# Gộp data sau one-hot
X_encode = pd.concat([X_origin.drop(columns=encode_cols), X_encoded_df], axis=1)

# Encode y
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_origin)

# Continuous columns cần scale
continuous_data = [
    'Age', 'DailyRate', 'DistanceFromHome', 'HourlyRate', 'MonthlyIncome',
    'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike',
    'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany',
    'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager'
]

# Standard Scaler
scaler = StandardScaler()
X_scaled_df = pd.DataFrame(
    scaler.fit_transform(X_encode[continuous_data]),
    columns=continuous_data,
    index=X_encode.index
).astype('float32')

# Gộp cùng dữ liệu còn lại
X_final = pd.concat(
    [
        X_encode.drop(columns=continuous_data),
        X_scaled_df
    ],
    axis=1
)

# One-hot columns cũng ép về float32
one_hot_cols = X_encoded_df.columns
X_final[one_hot_cols] = X_final[one_hot_cols].astype('float32')

# Có 2 loại dữ liệu:
# X_final là dữ liệu đã đc chuẩn hóa
# X_encode là dữ liệu chưa chuẩn hóa


df


print(f'Số bản ghi dữ liệu: {len(X)}')
print(f'Số cột dữ liệu (tập X): {len(X.columns)}')



X_origin_not_std = X_encode.copy()
y_origin = y.copy()


X_origin_not_std


y_encode = y_origin
y_encode





from collections import Counter

class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None, proba=None):
        self.feature = feature
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value
        self.proba = proba

    def is_leaf(self):
        return self.value is not None

class DecisionTree:
    def __init__(self, criterion="gini", max_depth=4, min_samples_leaf=25,
                 min_samples_split=50, random_state=42):
        self.criterion = criterion
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.random_state = random_state
        self.root = None

        np.random.seed(random_state)

    def fit(self, X, y):
        """Build the decision tree from training data"""
        if hasattr(X, 'values'):
            X = X.values
        if hasattr(y, 'values'):
            y = y.values.ravel()

        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)

        self.root = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        n_labels = len(np.unique(y))

        # Stopping criteria
        if (depth >= self.max_depth or
            n_labels == 1 or
            n_samples < self.min_samples_split):
            leaf_value = self._most_common_label(y)
            leaf_proba = self._calculate_class_probabilities(y)
            return Node(value=leaf_value, proba=leaf_proba)

        # Find best split
        best_feature, best_threshold = self._best_split(X, y, n_features)

        if best_feature is None:
            leaf_value = self._most_common_label(y)
            leaf_proba = self._calculate_class_probabilities(y)
            return Node(value=leaf_value, proba=leaf_proba)

        # Split data
        left_idxs = X[:, best_feature] <= best_threshold
        right_idxs = ~left_idxs

        # Check min_samples_leaf constraint
        if np.sum(left_idxs) < self.min_samples_leaf or np.sum(right_idxs) < self.min_samples_leaf:
            leaf_value = self._most_common_label(y)
            leaf_proba = self._calculate_class_probabilities(y)
            return Node(value=leaf_value, proba=leaf_proba)

        # Grow children recursively
        left = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)
        right = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)

        return Node(best_feature, best_threshold, left, right)

    def _best_split(self, X, y, n_features):
        best_score = float('inf') if self.criterion == "gini" else -1
        split_idx, split_threshold = None, None

        for feature_idx in range(n_features):
            X_column = X[:, feature_idx]
            thresholds = np.unique(X_column)

            for threshold in thresholds:
                left_idxs = X_column <= threshold
                right_idxs = ~left_idxs

                # Check min_samples_leaf
                if np.sum(left_idxs) < self.min_samples_leaf or np.sum(right_idxs) < self.min_samples_leaf:
                    continue

                if self.criterion == "gini":
                    score = self._gini_split(y, X_column, threshold)
                    if score < best_score:
                        best_score = score
                        split_idx = feature_idx
                        split_threshold = threshold
                else:  # entropy
                    score = self._information_gain(y, X_column, threshold)
                    if score > best_score:
                        best_score = score
                        split_idx = feature_idx
                        split_threshold = threshold

        return split_idx, split_threshold

    def _gini_split(self, y, X_column, threshold):
        """Calculate weighted Gini impurity for a split"""
        left_idxs = X_column <= threshold
        right_idxs = ~left_idxs

        if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:
            return float('inf')

        n = len(y)
        n_left, n_right = len(y[left_idxs]), len(y[right_idxs])

        gini_left = self._gini_impurity(y[left_idxs])
        gini_right = self._gini_impurity(y[right_idxs])

        weighted_gini = (n_left / n) * gini_left + (n_right / n) * gini_right
        return weighted_gini

    def _gini_impurity(self, y):
        """Calculate Gini impurity"""
        if len(y) == 0:
            return 0

        classes, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        gini = 1.0 - np.sum(probabilities ** 2)
        return gini

    def _information_gain(self, y, X_column, threshold):
        """Calculate information gain for a split"""
        parent_entropy = self._entropy(y)

        left_idxs = X_column <= threshold
        right_idxs = ~left_idxs

        if len(y[left_idxs]) == 0 or len(y[right_idxs]) == 0:
            return 0

        n = len(y)
        n_left, n_right = len(y[left_idxs]), len(y[right_idxs])
        e_left, e_right = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])
        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right

        ig = parent_entropy - child_entropy
        return ig

    def _entropy(self, y):
        """Calculate entropy"""
        if len(y) == 0:
            return 0

        classes, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])

    def _most_common_label(self, y):
        """Get most common label"""
        counter = Counter(y)
        return counter.most_common(1)[0][0]

    def _calculate_class_probabilities(self, y):
        """Calculate probability distribution over classes in leaf node"""
        proba = np.zeros(self.n_classes_)
        classes, counts = np.unique(y, return_counts=True)

        for c, count in zip(classes, counts):
            class_idx = np.where(self.classes_ == c)[0][0]
            proba[class_idx] = count / len(y)

        return proba

    def predict(self, X):
        """Predict class labels for samples in X"""
        if hasattr(X, 'values'):
            X = X.values
        return np.array([self._traverse_tree(x, self.root) for x in X])

    def predict_proba(self, X):
        """Predict class probabilities for samples in X"""
        if hasattr(X, 'values'):
            X = X.values
        return np.array([self._traverse_tree_proba(x, self.root) for x in X])

    def _traverse_tree(self, x, node):
        if node.is_leaf():
            return node.value

        if x[node.feature] <= node.threshold:
            return self._traverse_tree(x, node.left)
        return self._traverse_tree(x, node.right)

    def _traverse_tree_proba(self, x, node):
        if node.is_leaf():
            return node.proba

        if x[node.feature] <= node.threshold:
            return self._traverse_tree_proba(x, node.left)
        return self._traverse_tree_proba(x, node.right)


import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score

def evaluate(model, X_train, X_test, y_train, y_test):
    # 1. Huấn luyện mô hình
    model.fit(X_train, y_train)

    # 2. Dự đoán mặc định (Threshold = 0.5)
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    print("\n" + "="*40)
    print(" >>> DEFAULT THRESHOLD (0.5) RESULTS")
    print("="*40)

    # Kết quả trên tập TRAIN
    print("\n--- TRAINING SET (Default) ---")
    print("Confusion Matrix:")
    print(confusion_matrix(y_train, y_train_pred))
    print(f"Accuracy: {accuracy_score(y_train, y_train_pred):.4f}")
    print(pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True)).T)

    # Kết quả trên tập TEST
    print("\n--- TEST SET (Default) ---")
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_test_pred))
    print(f"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}")
    print(pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True)).T)

    # 3. Tìm threshold tối ưu dựa trên tập TRAIN
    print("\n" + "="*40)
    print(" >>> TUNING THRESHOLD (Based on TRAINING Data)")
    print("="*40)

    # Lấy xác suất dự đoán trên tập TRAIN
    y_train_proba = model.predict_proba(X_train)[:, 1]
    y_test_proba = model.predict_proba(X_test)[:, 1]

    best_threshold = 0.5
    best_f1 = 0.0

    # Duyệt qua các ngưỡng để tìm F1 cao nhất trên tập TRAIN
    for threshold in np.arange(0.1, 0.9, 0.05):
        y_train_pred_th = (y_train_proba >= threshold).astype(int)
        f1 = f1_score(y_train, y_train_pred_th)

        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold

    print(f"Optimal Threshold found on Train: {best_threshold:.2f}")
    print(f"Max Training F1-score: {best_f1:.4f}")

    # 4. Áp dụng threshold tối ưu vào tập TEST
    y_test_pred_opt = (y_test_proba >= best_threshold).astype(int)

    print("\n" + "="*40)
    print(f" >>> TEST RESULTS WITH OPTIMAL THRESHOLD ({best_threshold:.2f})")
    print("="*40)
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_test_pred_opt))
    print(f"Accuracy: {accuracy_score(y_test, y_test_pred_opt):.4f}")

    report_df = pd.DataFrame(classification_report(y_test, y_test_pred_opt, output_dict=True)).T
    print(report_df)











from sklearn.model_selection import train_test_split

X_encode_copy = X_origin_not_std
y_encode_copy = y_encode



X_encode_copy


y_encode_copy


X_train_origin_t1, X_test_origin_t1, y_train_origin_t1, y_test_origin_t1 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.2, random_state=42, stratify=y_encode_copy
)

X_train_origin_t2, X_test_origin_t2, y_train_origin_t2, y_test_origin_t2 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.3, random_state=42, stratify=y_encode_copy
)

X_train_origin_t3, X_test_origin_t3, y_train_origin_t3, y_test_origin_t3 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.4, random_state=42, stratify=y_encode_copy
)


# Test 1: GINI
rf_gini = DecisionTree(
    max_depth=4,
    min_samples_split=50,
    min_samples_leaf=25,     # Mỗi leaf ít nhất 15 samples
    criterion='gini',
    random_state=42
)


evaluate(rf_gini, X_train_origin_t1, X_test_origin_t1, y_train_origin_t1, y_test_origin_t1)


evaluate(rf_gini, X_train_origin_t2, X_test_origin_t2, y_train_origin_t2, y_test_origin_t2)


evaluate(rf_gini, X_train_origin_t3, X_test_origin_t3, y_train_origin_t3, y_test_origin_t3)





from sklearn.tree import DecisionTreeClassifier

# Test 1: GINI
rf_gini_skl = DecisionTreeClassifier(
    max_depth=4,
    min_samples_split=50,
    min_samples_leaf=25,
    criterion='gini',
    random_state=42
)



evaluate(rf_gini_skl, X_train_origin_t1, X_test_origin_t1, y_train_origin_t1, y_test_origin_t1)


evaluate(rf_gini_skl, X_train_origin_t2, X_test_origin_t2, y_train_origin_t2, y_test_origin_t2)



evaluate(rf_gini_skl, X_train_origin_t3, X_test_origin_t3, y_train_origin_t3, y_test_origin_t3)


















from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler
import pandas as pd

df = pd.read_csv('../Dataset/archive/WA_Fn-UseC_-HR-Employee-Attrition.csv')

X = df.drop(['EmployeeNumber', 'Over18', 'EmployeeCount', 'StandardHours'], axis=1)
y = df['Attrition']

col_int32 = ['DailyRate', 'MonthlyIncome', 'MonthlyRate']
col_int16 = ['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate',
             'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'NumCompaniesWorked',
             'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction',
             'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',
             'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',
             'YearsSinceLastPromotion', 'YearsWithCurrManager']

# ép kiểu
X[col_int32] = X[col_int32].astype('int32')
X[col_int16] = X[col_int16].astype('int16')

X_origin = X.copy()
y_origin = y.copy()

# One-hot cho các cột nominal
one_hot_encoder = OneHotEncoder(
    sparse_output=False,
    handle_unknown='ignore',
    drop='first'
)

encode_cols = [
    'BusinessTravel', 'Department', 'EducationField',
    'Gender', 'JobRole', 'MaritalStatus', 'OverTime'
]

X_encoded_array = one_hot_encoder.fit_transform(X_origin[encode_cols])
encoded_columns = one_hot_encoder.get_feature_names_out(encode_cols)

X_encoded_df = pd.DataFrame(
    X_encoded_array,
    columns=encoded_columns,
    index=X_origin.index
)

# Gộp data sau one-hot
X_encode = pd.concat([X_origin.drop(columns=encode_cols), X_encoded_df], axis=1)

# Encode y
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y_origin)


X_encode


X_encode_copy = X_encode.copy()


X_encode_copy['Attrition'] = X_encode_copy['Attrition'].map({'Yes':1, 'No':0})


X_encode_copy


X_encode_copy.drop('Attrition', axis=1).corrwith(X_encode_copy.Attrition).sort_values().plot(kind='barh', figsize=(5, 10))


feature_correlation = X_encode_copy.drop('Attrition', axis=1).corrwith(X_encode_copy.Attrition).sort_values()
model_col = feature_correlation[np.abs(feature_correlation) > 0.02].index
len(model_col)


X_encode_copy = X_encode_copy[model_col]


X_encode_copy


from sklearn.model_selection import train_test_split

X_train_origin_t1, X_test_origin_t1, y_train_origin_t1, y_test_origin_t1 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.2, random_state=42, stratify=y_encode_copy
)

X_train_origin_t2, X_test_origin_t2, y_train_origin_t2, y_test_origin_t2 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.3, random_state=42, stratify=y_encode_copy
)

X_train_origin_t3, X_test_origin_t3, y_train_origin_t3, y_test_origin_t3 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.4, random_state=42, stratify=y_encode_copy
)






# Test 1: GINI
rf_gini = DecisionTree(
    max_depth=4,
    min_samples_split=50,
    min_samples_leaf=25,     # Mỗi leaf ít nhất 15 samples
    criterion='gini',
    random_state=42
)


evaluate(rf_gini, X_train_origin_t1, X_test_origin_t1, y_train_origin_t1, y_test_origin_t1)


evaluate(rf_gini, X_train_origin_t2, X_test_origin_t2, y_train_origin_t2, y_test_origin_t2)


evaluate(rf_gini, X_train_origin_t3, X_test_origin_t3, y_train_origin_t3, y_test_origin_t3)





from sklearn.tree import DecisionTreeClassifier

# Test 1: GINI
rf_gini_skl = DecisionTreeClassifier(
    max_depth=4,
    min_samples_split=50,
    min_samples_leaf=25,
    criterion='gini',
    random_state=42
)



evaluate(rf_gini_skl, X_train_origin_t1, X_test_origin_t1, y_train_origin_t1, y_test_origin_t1)


evaluate(rf_gini_skl, X_train_origin_t2, X_test_origin_t2, y_train_origin_t2, y_test_origin_t2)



evaluate(rf_gini_skl, X_train_origin_t3, X_test_origin_t3, y_train_origin_t3, y_test_origin_t3)






from sklearn.tree import DecisionTreeClassifier

# Test 1: GINI
rf_gini_skl = DecisionTreeClassifier(
    max_depth=4,
    min_samples_split=50,
    min_samples_leaf=25,
    criterion='gini',
    random_state=42
)



from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# Khởi tạo SMOTE
smote = SMOTE(random_state=42)

X_train_origin_t1, X_test_origin_t1, y_train_origin_t1, y_test_origin_t1 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.2, random_state=42, stratify=y_encode_copy
)
X_train_smote_t1, y_train_smote_t1 = smote.fit_resample(X_train_origin_t1, y_train_origin_t1)
print(f"Case 1 (0.2) - Original Train: {y_train_origin_t1.shape}, SMOTE Train: {y_train_smote_t1.shape}")


X_train_origin_t2, X_test_origin_t2, y_train_origin_t2, y_test_origin_t2 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.3, random_state=42, stratify=y_encode_copy
)
X_train_smote_t2, y_train_smote_t2 = smote.fit_resample(X_train_origin_t2, y_train_origin_t2)
print(f"Case 2 (0.3) - Original Train: {y_train_origin_t2.shape}, SMOTE Train: {y_train_smote_t2.shape}")


X_train_origin_t3, X_test_origin_t3, y_train_origin_t3, y_test_origin_t3 = train_test_split(
    X_encode_copy, y_encode_copy, test_size=0.4, random_state=42, stratify=y_encode_copy
)
X_train_smote_t3, y_train_smote_t3 = smote.fit_resample(X_train_origin_t3, y_train_origin_t3)
print(f"Case 3 (0.4) - Original Train: {y_train_origin_t3.shape}, SMOTE Train: {y_train_smote_t3.shape}")


evaluate(rf_gini_skl, X_train_smote_t1, X_test_origin_t1, y_train_smote_t1, y_test_origin_t1)


evaluate(rf_gini_skl, X_train_smote_t2, X_test_origin_t2, y_train_smote_t2, y_test_origin_t2)


evaluate(rf_gini_skl, X_train_smote_t3, X_test_origin_t3, y_train_smote_t3, y_test_origin_t3)
































from sklearn.tree import DecisionTreeClassifier

rf_gini_skl = DecisionTreeClassifier(
    max_depth=4,
    min_samples_split=50,
    min_samples_leaf=25,
    criterion='gini',
    random_state=42
)
rf_gini = DecisionTree(
    max_depth=4,
    min_samples_split=50,
    min_samples_leaf=25,     # Mỗi leaf ít nhất 15 samples
    criterion='gini',
    random_state=42
)
rf_gini_skl.fit(X_train_origin_t3, y_train_origin_t3)
rf_gini.fit(X_train_origin_t3, y_train_origin_t3)


import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score

y_scores = rf_gini.predict_proba(X_test_origin_t3)[:, 1]

# Tính toán các chỉ số
precisions, recalls, pr_thresholds = precision_recall_curve(y_test_origin_t3, y_scores)
fpr, tpr, roc_thresholds = roc_curve(y_test_origin_t3, y_scores)
auc_score = roc_auc_score(y_test_origin_t3, y_scores)

target_threshold = 0.25

# 1. Tìm index gần 0.25 nhất trong mảng pr_thresholds
idx_pr = np.argmin(np.abs(pr_thresholds - target_threshold))
p_at_th = precisions[idx_pr]
r_at_th = recalls[idx_pr]

# 2. Tìm index gần 0.25 nhất trong mảng roc_thresholds
idx_roc = np.argmin(np.abs(roc_thresholds - target_threshold))
fpr_at_th = fpr[idx_roc]
tpr_at_th = tpr[idx_roc]

plt.figure(figsize=(16, 5))

# Biểu đồ 1: Precision-Recall vs Threshold
plt.subplot(1, 3, 1)
plt.plot(pr_thresholds, precisions[:-1], label="Precision", color='blue')
plt.plot(pr_thresholds, recalls[:-1], label="Recall", color='orange')
plt.axvline(x=target_threshold, color='red', linestyle='--', label=f'Threshold {target_threshold}')
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision - Recall vs Threshold")
plt.legend()
plt.grid(True, alpha=0.3)

# Biểu đồ 2: Precision-Recall Curve
plt.subplot(1, 3, 2)
plt.plot(recalls, precisions, label='PR Curve')
plt.scatter(r_at_th, p_at_th, color='red', s=100, zorder=5, label=f'Th={target_threshold}')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid(True, alpha=0.3)

# Biểu đồ 3: ROC Curve
plt.subplot(1, 3, 3)
plt.plot(fpr, tpr, label=f'ROC (AUC = {auc_score:.3f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.scatter(fpr_at_th, tpr_at_th, color='red', s=100, zorder=5, label=f'Th={target_threshold}')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()








import matplotlib.pyplot as plt
import numpy as np

def plot_prediction_comparison(model,
                               X_train_val, y_train_val,
                               X_test, y_test,
                               threshold=0.5):

    # --- 1. CHUẨN HÓA DỮ LIỆU ĐẦU VÀO (Tránh lỗi Pandas Series) ---
    # Chuyển hết về numpy array để index hoạt động đúng
    y_train_val = np.array(y_train_val)
    y_test = np.array(y_test)

    # --- 2. DỰ ĐOÁN ---
    # Train/Val
    y_train_proba = model.predict_proba(X_train_val)[:, 1]
    y_train_pred = (y_train_proba >= threshold).astype(int)

    # Test
    y_test_proba = model.predict_proba(X_test)[:, 1]
    y_test_pred = (y_test_proba >= threshold).astype(int)

    # --- 3. VẼ BIỂU ĐỒ ---
    plt.figure(figsize=(16, 6))

    plt.subplot(1, 2, 1)
    idx_train = np.arange(len(y_train_val))

    # Tìm index đúng/sai
    correct_train = (y_train_pred == y_train_val)
    wrong_train = (y_train_pred != y_train_val)

    # Vẽ điểm (thêm alpha để nhìn xuyên thấu các điểm đè nhau)
    plt.scatter(idx_train[correct_train], y_train_val[correct_train],
                color='green', label='Đúng', s=30, alpha=0.6)
    plt.scatter(idx_train[wrong_train], y_train_val[wrong_train],
                color='red', label='Sai', s=30, marker='x', alpha=0.8)

    plt.title(f"Tập TRAIN/VAL (Threshold={threshold})")
    plt.xlabel("Index mẫu")
    plt.ylabel("Nhãn thực tế (0/1)")
    plt.yticks([0, 1]) # Chỉ hiện số 0 và 1 ở trục Y
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.5)

    # ====== SUBPLOT 2: TEST ======
    plt.subplot(1, 2, 2)
    idx_test = np.arange(len(y_test))

    # Tìm index đúng/sai
    correct_test = (y_test_pred == y_test)
    wrong_test = (y_test_pred != y_test)

    plt.scatter(idx_test[correct_test], y_test[correct_test],
                color='green', label='Đúng', s=30, alpha=0.6)
    plt.scatter(idx_test[wrong_test], y_test[wrong_test],
                color='red', label='Sai', s=30, marker='x', alpha=0.8)

    plt.title(f"Tập TEST (Threshold={threshold})")
    plt.xlabel("Index mẫu")
    plt.yticks([0, 1])
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.5)

    plt.tight_layout()
    plt.show()


plot_prediction_comparison(
    model=rf_gini_skl   ,
    X_train_val=X_train_origin_t3,
    y_train_val=y_train_origin_t3,
    X_test=X_test_origin_t3,
    y_test=y_test_origin_t3,
    threshold=0.25
)






